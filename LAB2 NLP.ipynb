{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Experiment 2\n",
    "## 29th May 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happi'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEMMING OF WORDS USING PORTER STEMMER\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmerporter = PorterStemmer()\n",
    "stemmerporter.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'punish'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmerporter.stem('punishment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'happy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEMMING OF WORDS USING LANCASTER STEMMER\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "stemmerLan = LancasterStemmer()\n",
    "stemmerLan.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parl'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEMMING OF WORDS USING SNOWBALL STEMMER\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "frenchstemmer = SnowballStemmer('french')\n",
    "frenchstemmer.stem('parle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEMMING OF WORDS USING REGEXP STEMMER\n",
    "import nltk\n",
    "from nltk.stem import RegexpStemmer\n",
    "stemmerregexp = RegexpStemmer('learn')\n",
    "stemmerregexp.stem('learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: STEMMING PARAGRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A comput is a machin that can be instruct to carri out sequenc of arithmet or logic oper automat via comput programming.\n"
     ]
    }
   ],
   "source": [
    "#USING PORTERSTEMMER\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "example = \"A computer is a machine that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming.\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print(\" \".join(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'regexp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-114ad9db59a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mstemmer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpStemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"A computer is a machine that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'regexp'"
     ]
    }
   ],
   "source": [
    "#USING REGEXP STEMMER\n",
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "stemmer = RegexpStemmer()\n",
    "example = \"A computer is a machine that can be instructed to carry out sequences of arithmetic or logical operations automatically via computer programming.\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print(\" \".join(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suis rav de vous rencontr\n"
     ]
    }
   ],
   "source": [
    "#USING SNOWBALL STEMMER\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages\n",
    "\n",
    "stemmer = SnowballStemmer('french')\n",
    "example = \"Je suis ravi de vous rencontrer\"\n",
    "example = [stemmer.stem(token) for token in example.split(\" \")]\n",
    "print(\" \".join(example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: LEMMATIZER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Books\n",
      "sheep\n",
      "goose\n",
      "tasty\n",
      "Am\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"Books\"))\n",
    "print(lemmatizer.lemmatize(\"sheep\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"tastier\",pos='a')) #given the part-of-speech, better lemmatizes to good\n",
    "print(lemmatizer.lemmatize(\"Am\")) #This error is fixed when the part of speech is given\n",
    "print(lemmatizer.lemmatize(\"am\", pos = 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: CHINESE SEGMENTATION USING JIEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\JEV\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.684 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "儿童 通常 在 两岁 的 时候 开始 说话 。\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "seg = jieba.cut(\"儿童通常在两岁的时候开始说话。\", cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 跟 你 讲\n"
     ]
    }
   ],
   "source": [
    "seg = jieba.cut(\"我跟你讲\", cut_all = True)\n",
    "print(\" \".join(seg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: BASIC TEXT PROCESSING PIPELINE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Be', 'a', 'good', 'human', 'to', 'all']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sent = \"Be a good human to all\"\n",
    "#BREAKS THEM IN TO SMALLER CHUNKS\n",
    "words = nltk.word_tokenize(sent)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Between', 'May', '31', 'and', 'June', '11', ',', 'Haryana', 'as', 'a', 'whole', 'recorded', 'a', 'three-fold', 'increase', 'in', 'coronavirus', 'deaths', 'and', 'cases', ',', 'from', '20', 'to', '64', 'and', '2,091', 'to', '5,968', 'respectively']\n",
      "[('Between', 'NNP'), ('May', 'NNP'), ('31', 'CD'), ('and', 'CC'), ('June', 'NNP'), ('11', 'CD'), (',', ','), ('Haryana', 'NNP'), ('as', 'IN'), ('a', 'DT'), ('whole', 'NN'), ('recorded', 'VBD'), ('a', 'DT'), ('three-fold', 'JJ'), ('increase', 'NN'), ('in', 'IN'), ('coronavirus', 'JJ'), ('deaths', 'NNS'), ('and', 'CC'), ('cases', 'NNS'), (',', ','), ('from', 'IN'), ('20', 'CD'), ('to', 'TO'), ('64', 'CD'), ('and', 'CC'), ('2,091', 'CD'), ('to', 'TO'), ('5,968', 'CD'), ('respectively', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "texts = [''''Gurgaon in Haryana, one of the worst-hit districts in the country by the coronavirus pandemic, has recorded a six-fold increase in deaths, while the cases have gone up more than three-and-a-half times between May 31 and June 11, official data showed on Friday.\n",
    "Of the nearly 6,000 total cases in Haryana, Gurgaon alone accounts for more than 45 per cent and 19 out of the total 64 COVID-19 related deaths in the state.\n",
    "\n",
    "The two other worst-hit districts are - Faridabad and Sonipat, which, like Gurgaon, fall in the National Capital Region next to Delhi. Together, these three districts account for more than 4,000 cases and 46 of the 64 deaths as on June 11.\n",
    "\n",
    "By May 31, Gurgaon had three deaths and 774 COVID-19 cases (487 active). But by June 11, the numbers rose to 19 fatalities and 2,737 cases (1,760 active).\n",
    "\n",
    "Gurgaon reported six deaths on a single day on June 11, the health department data showed.\n",
    "\n",
    "Between May 31 and June 11, Haryana as a whole recorded a three-fold increase in coronavirus deaths and cases, from 20 to 64 and 2,091 to 5,968 respectively''']\n",
    "for text in texts:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Between', 'NNP'), ('May', 'NNP'), ('31', 'CD'), ('and', 'CC'), ('June', 'NNP'), ('11', 'CD'), (',', ','), ('Haryana', 'NNP'), ('as', 'IN'), ('a', 'DT'), ('whole', 'NN'), ('recorded', 'VBD'), ('a', 'DT'), ('three-fold', 'JJ'), ('increase', 'NN'), ('in', 'IN'), ('coronavirus', 'JJ'), ('deaths', 'NNS'), ('and', 'CC'), ('cases', 'NNS'), (',', ','), ('from', 'IN'), ('20', 'CD'), ('to', 'TO'), ('64', 'CD'), ('and', 'CC'), ('2,091', 'CD'), ('to', 'TO'), ('5,968', 'CD'), ('respectively', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(words)\n",
    "print(tagged)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
