{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5\n",
    "## Cosine Similiarity Between Two Documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectoriser and cosine similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1=['''Natural Language Processing (NLP) is all about leveraging tools, techniques and algorithms to process and understand natural language-based data, which is usually unstructured like text, speech and so on. In this series of articles, we will be looking at tried and tested strategies, techniques and workflows which can be leveraged by practitioners and data scientists to extract useful insights from text data. We will also cover some useful and interesting use-cases for NLP. This article will be all about processing and understanding text data with tutorials and hands-on examples. am assuming you are aware of the CRISP-DM model, which is typically an industry standard for executing any data science project. Typically, any NLP-based problem can be solved by a methodical workflow that has a sequence of steps. The major steps are depicted in the following figure.\n",
    "We usually start with a corpus of text documents and follow standard processes of text wrangling and pre-processing, parsing and basic exploratory data analysis. Based on the initial insights, we usually represent the text using relevant feature engineering techniques. Depending on the problem at hand, we either focus on building predictive supervised models or unsupervised models, which usually focus more on pattern mining and grouping. Finally, we evaluate the model and the overall success criteria with relevant stakeholders or customers, and deploy the final model for future usage.''']\n",
    "\n",
    "s2=['''Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.\n",
    "The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\n",
    "Most NLP techniques rely on machine learning to derive meaning from human languages.''']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the transform\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 52, 'language': 42, 'processing': 63, 'nlp': 53, 'leveraging': 44, 'tools': 83, 'techniques': 80, 'algorithms': 0, 'process': 61, 'understand': 87, 'based': 6, 'data': 15, 'usually': 95, 'unstructured': 89, 'like': 45, 'text': 82, 'speech': 72, 'series': 70, 'articles': 3, 'looking': 46, 'tried': 84, 'tested': 81, 'strategies': 77, 'workflows': 97, 'leveraged': 43, 'practitioners': 57, 'scientists': 68, 'extract': 26, 'useful': 93, 'insights': 40, 'cover': 11, 'interesting': 41, 'use': 92, 'cases': 9, 'article': 2, 'understanding': 88, 'tutorials': 85, 'hands': 37, 'examples': 23, 'assuming': 4, 'aware': 5, 'crisp': 12, 'dm': 19, 'model': 50, 'typically': 86, 'industry': 38, 'standard': 74, 'executing': 24, 'science': 67, 'project': 64, 'problem': 60, 'solved': 71, 'methodical': 48, 'workflow': 96, 'sequence': 69, 'steps': 76, 'major': 47, 'depicted': 17, 'following': 33, 'figure': 28, 'start': 75, 'corpus': 10, 'documents': 20, 'follow': 32, 'processes': 62, 'wrangling': 98, 'pre': 58, 'parsing': 55, 'basic': 7, 'exploratory': 25, 'analysis': 1, 'initial': 39, 'represent': 66, 'using': 94, 'relevant': 65, 'feature': 27, 'engineering': 21, 'depending': 16, 'hand': 36, 'focus': 31, 'building': 8, 'predictive': 59, 'supervised': 79, 'models': 51, 'unsupervised': 90, 'pattern': 56, 'mining': 49, 'grouping': 35, 'finally': 30, 'evaluate': 22, 'overall': 54, 'success': 78, 'criteria': 13, 'stakeholders': 73, 'customers': 14, 'deploy': 18, 'final': 29, 'future': 34, 'usage': 91}\n"
     ]
    }
   ],
   "source": [
    "# summarize\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode document\n",
    "vector = vectorizer.transform(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 99)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1\n",
      "  1 1 1 1 2 1 2 1 1 1 1 1 1 1 3 2 2 3 1 1 1 1 1 1 2 1 1 3 1 2 1 1 1 1 1 1\n",
      "  1 1 2 1 2 1 1 1 3 1 6 1 1 1 2 1 1 1 1 1 1 2 1 4 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "vec1=vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'natural': 52, 'language': 42, 'processing': 63, 'nlp': 53, 'leveraging': 44, 'tools': 83, 'techniques': 80, 'algorithms': 0, 'process': 61, 'understand': 87, 'based': 6, 'data': 15, 'usually': 95, 'unstructured': 89, 'like': 45, 'text': 82, 'speech': 72, 'series': 70, 'articles': 3, 'looking': 46, 'tried': 84, 'tested': 81, 'strategies': 77, 'workflows': 97, 'leveraged': 43, 'practitioners': 57, 'scientists': 68, 'extract': 26, 'useful': 93, 'insights': 40, 'cover': 11, 'interesting': 41, 'use': 92, 'cases': 9, 'article': 2, 'understanding': 88, 'tutorials': 85, 'hands': 37, 'examples': 23, 'assuming': 4, 'aware': 5, 'crisp': 12, 'dm': 19, 'model': 50, 'typically': 86, 'industry': 38, 'standard': 74, 'executing': 24, 'science': 67, 'project': 64, 'problem': 60, 'solved': 71, 'methodical': 48, 'workflow': 96, 'sequence': 69, 'steps': 76, 'major': 47, 'depicted': 17, 'following': 33, 'figure': 28, 'start': 75, 'corpus': 10, 'documents': 20, 'follow': 32, 'processes': 62, 'wrangling': 98, 'pre': 58, 'parsing': 55, 'basic': 7, 'exploratory': 25, 'analysis': 1, 'initial': 39, 'represent': 66, 'using': 94, 'relevant': 65, 'feature': 27, 'engineering': 21, 'depending': 16, 'hand': 36, 'focus': 31, 'building': 8, 'predictive': 59, 'supervised': 79, 'models': 51, 'unsupervised': 90, 'pattern': 56, 'mining': 49, 'grouping': 35, 'finally': 30, 'evaluate': 22, 'overall': 54, 'success': 78, 'criteria': 13, 'stakeholders': 73, 'customers': 14, 'deploy': 18, 'final': 29, 'future': 34, 'usage': 91}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 3 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#vectorise second document\n",
    "vector = vectorizer.transform(s2)\n",
    "print(vector.toarray())\n",
    "vec2=vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38567376]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cosine_similarity(vec1,vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(vec1,vec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESULT\n",
    "\n",
    "### Here the 2 documents we provided are having 38 as cosine similarity therefor it is closer to 0 than 1 hence this documents can be said to DISIMILAR means the vectors are perpendicular and not parallel but closer to Perpendicular)\n",
    "\n",
    "## Rules\n",
    "\n",
    "### If the cosine similarity is 1 ,then the agngles value is 0 degree and the vectors are said to be parallel\n",
    "\n",
    "### If the cosine value is 0 and value of the angle is 90 degrees ,then the vectors are said to be perpendicular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
